---
title: "Project text mining"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, warning=FALSE}
#load the package
library(qdap)
library(tidyr)
library(dplyr)
library(tm)
library(NLP)
library(textmineR)
library(RWeka)
library(wordcloud)
library(e1071)
library(caret)
library(magrittr)
library(gtools)
```

## R Markdown

```{r, echo=FALSE,warning=FALSE}
raw_data <- read.csv("CX Wave 1 Merge file 2.22.18.csv",stringsAsFactors = FALSE,na.strings=c("",NA, " "))

head(raw_data, 5)

target_data <- raw_data[,c(420,421)]

text_data <- raw_data[,c(198:210,235:238,246,254,262,362:363)]

text_data[which(is.na(text_data)),]<- " "

new_data <- cbind(target_data,text_data)

new_data$all_text <- apply(raw_data[,c(198:210,235:238,246,254,262,362:363)], 1, paste, collapse = ",")


#just keep all open-ends and target variables
new_data <- new_data[,-c(3:24)]

#make a source vector
text <- VectorSource(new_data$all_text)

text_corpus <- VCorpus(text)

clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "the", "and","i", "Planet","Fitness","NA","gym"))
  corpus <- tm_map(corpus, content_transformer(tolower))
  return(corpus)
}

tokenizer <- function(x) 
  NGramTokenizer(x, Weka_control(min = 3, max = 3))

text_corpus <- clean_corpus(text_corpus)

str(text_corpus)

text_dtm <- DocumentTermMatrix(text_corpus, control = list(tokenize = tokenizer))

str(text_dtm)


text_dtm_m <- as.matrix(text_dtm)

text_freq <- rowSums(text_dtm_m)

wordcloud(names(text_freq), text_freq, max.words = 25, color = "blue" )

#inspect(text_tdm[1:4, 30:35])

#remove words with the less frequent terms such that the sparsity is less than 0.95
text_dtm <- removeSparseTerms(text_dtm, 0.99)

#creat word cloud for churn member
churn_index <- which(new_data$Survival == 1)
churn_index[1:3]

retain_index <-which(new_data$Survival == 0)
churn_index[1:3]

#create word cloud for churn member
wordcloud(text_corpus[churn_index], min.freq=80)

#create word clud for retain member
wordcloud(text_corpus[retain_index], min.freq=80)

#prepare the data
set.seed(1234)

n <- nrow(new_data)

n_train <- 0.8*n

train_indices <- sample(1:n, n_train)

#subset the data frame to training indices only
new_data_train <- new_data[train_indices, ]

# exclude the training indices to create the test set
new_data_test <- new_data[-train_indices, ]

text_corpus_train <-text_corpus[train_indices]
text_corpus_test <-text_corpus[-train_indices]

text_dtm_train <- text_dtm[train_indices,]
text_dtm_test <- text_dtm[-train_indices,]

#separate training data to churn and retain
churn <- subset(new_data_train,Survival == 1)
retain <- subset(new_data_train, Survival == 0)

#build naive bayes model

#identify high frequeny words 5+
five_times_words <- findFreqTerms(text_dtm_train,5)
length(five_times_words)

ten_times_words[1:5]

#Create document-term matrices using frequent words
text_train <- DocumentTermMatrix(text_corpus_train, control=list(dictionary = five_times_words))

text_test <- DocumentTermMatrix(text_corpus_test, control=list(dictionary = five_times_words))

#Convert count information to "Yes", "No"
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

text_train <- apply(text_train, 2, convert_count)
text_test <- apply(text_test, 2, convert_count)

#Create a Naive Bayes classifier object
text_classifier <- naiveBayes(text_train, factor(new_data_train$Survival))

class(text_classifier)

text_test_pred <- predict(text_classifier, newdata=new_data_test)

confusionMatrix(new_data_test$Survival, text_test_pred)


```
```{r}
#sentiment analysis

#compute the polarity score for each obs
text_m <- as.matrix(text_dtm)
text_df <- as.data.frame(text_m)
polarity_score <- polarity(text_df)


```

