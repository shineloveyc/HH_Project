---
title: "Project text mining"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## load library
```{r, warning=FALSE}
#load the package
library(qdap)
library(tidyr)
library(dplyr)
library(tm)
library(NLP)
library(textmineR)
library(RWeka)
library(wordcloud)
library(e1071)
library(caret)
library(magrittr)
library(gtools)
library(ggthemes)
```

## R Markdown

```{r, echo=FALSE,warning=FALSE}
#get PF dataset
raw_data <- read.csv("CX Wave 1 Merge file 2.22.18.csv",stringsAsFactors = FALSE)

head(raw_data, 5)

#define dependent variables
target_data <- raw_data[,c(420,421)]

#define open-ended question

text_data <- raw_data[,c(198:210,235:238,246,254,262,362:363)]

#combine two datasest
new_data <- cbind(target_data,text_data)

#merge all text to one column
new_data$all_text <- apply(raw_data[,c(198:210,235:238,246,254,262,362:363)], 1, paste, collapse = " ")


#just keep all open-ends and target variables
new_data <- new_data[,-c(3:24)]

#convert Survival to factor
new_data$Survival<- factor(new_data$Survival, levels = c(0, 1))
```


## sentiment analysis
```{r, warning=FALSE}
#sentiment analysis

#make a source vector
open_end <- VectorSource(new_data$all_text)

#covert to corpus
open_end_corpus <- VCorpus(open_end)

#create polarity object of all text
open_end_pol <- polarity(new_data$all_text)

summary(open_end_pol$all$polarity)

#add polarity column
new_data_with_pol <- new_data %>% mutate(polarity = scale(open_end_pol$all$polarity))

#check missing value
sapply(new_data_with_pol, function(x) sum(is.na(x)))

#process the missing data
t.test(new_data_with_pol$polarity~new_data_with_pol$Survival)

#impute missing data
new_data_with_pol$polarity[is.nan(new_data_with_pol$polarity)] <- mean(new_data_with_pol$polarity, na.rm = TRUE)

pos_comments <- subset(new_data_with_pol$all_text, new_data_with_pol$polarity > 0)

# Subset negative comments
neg_comments <- subset(new_data_with_pol$all_text, new_data_with_pol$polarity < 0)

#paste and collapse the positive comments
pos_terms <- paste(pos_comments, collapse = " ")

# Paste and collapse the negative comments
neg_terms <- paste(neg_comments, collapse = " ")

# Concatenate the terms
all_terms <- c(pos_terms, neg_terms)

# Pipe a VectorSource Corpus
all_corpus <- all_terms %>% 
  VectorSource() %>% 
  VCorpus()

tokenizer <- function(x) 
  NGramTokenizer(x, Weka_control(min = 2, max = 2))

# Simple TFIDF TDM
all_tdm <- TermDocumentMatrix(
  all_corpus, 
  control = list(
    weighting = weightTfIdf, 
    removePunctuation = TRUE, 
    stopwords = stopwords(kind = "en")
  )
)

# Examine the TDM
all_tdm

# Matrix
all_tdm_m <- as.matrix(all_tdm)

# Column names
colnames(all_tdm_m) <- c("positive", "negative")

# Top pos words
order_by_pos <- order(all_tdm_m[, 1], decreasing = TRUE)

# Review top 10 pos words
all_tdm_m[order_by_pos, ] %>% head(n=20)

# Top neg words
order_by_neg <- order(all_tdm_m[, 2], decreasing = TRUE)

# Review top 10 neg words
all_tdm_m[order_by_neg, ] %>% head(n =20)

# Comparison cloud
comparison.cloud(
  all_tdm_m, 
  max.words = 50,
  colors = c("darkgreen","darkred")
)

```

## Textmining for churn and retain members
```{r, warning=FALSE}
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "Planet","Fitness","gym"))
  corpus <- tm_map(corpus, content_transformer(tolower))
  return(corpus)
}

#clean corpus
text_corpus <- clean_corpus(open_end_corpus)

#create a clean TDM

text_dtm <- DocumentTermMatrix(text_corpus)

#creat word cloud for churn member
churn_index <- which(new_data$Survival == 1)


retain_index <-which(new_data$Survival == 0)


#create word cloud for churn member
wordcloud(text_corpus[churn_index], min.freq=80,max.words = 50)

#create word clud for retain member
wordcloud(text_corpus[retain_index], min.freq=80,max.words = 50)
```

#build prediction model
```{r}
#prepare the data
set.seed(1234)

n <- nrow(new_data_with_pol)

n_train <- 0.8*n

train_indices <- sample(1:n, n_train)

#subset the data frame to training indices only
new_data_train <- new_data_with_pol[train_indices, ]

# exclude the training indices to create the test set
new_data_test <- new_data_with_pol[-train_indices, ]

text_corpus_train <-text_corpus[train_indices]
text_corpus_test <-text_corpus[-train_indices]

text_dtm_train <- text_dtm[train_indices,]
text_dtm_test <- text_dtm[-train_indices,]
```


## build logit model based on polarity score
```{r}
table(new_data_with_pol$Survival)

new_data_train_subset <- new_data_train[c(1,2,4)]

logit_model <- glm (Survival ~ ., data = new_data_train_subset, family = binomial(link="logit"))
summary(logit_model)

new_data_test_subset <- new_data_test[c(1,2,4)]

predict <- predict(logit_model,new_data_test_subset, type = "response")

pred_num <- ifelse(predict > 0.5, 1, 0)
pred <- factor(pred_num, levels=c(0, 1))
act <- new_data_test$Survival

mean(pred == act) 

```

## build naive bayes model based on word frequency
```{r}
#separate training data to churn and retain
churn <- subset(new_data_train,Survival == 1)
retain <- subset(new_data_train, Survival == 0)

#identify high frequeny words 5+
five_times_words <- findFreqTerms(text_dtm_train,5)
length(five_times_words)

#Create document-term matrices using frequent words
text_train <- DocumentTermMatrix(text_corpus_train, control=list(dictionary = five_times_words))

text_test <- DocumentTermMatrix(text_corpus_test, control=list(dictionary = five_times_words))


#Convert count information to "Yes", "No"
convert_count <- function(x) {
  y <- ifelse(x > 0, 1,0)
  y <- factor(y, levels=c(0,1), labels=c("No", "Yes"))
  y
}

text_train <- apply(text_train, 2, convert_count)
text_test <- apply(text_test, 2, convert_count)


#Create a Naive Bayes classifier object
text_classifier <- naiveBayes(text_train, factor(new_data_train$Survival))

class(text_classifier)

text_test_pred <- predict(text_classifier, newdata=new_data_test)

confusionMatrix(new_data_test$Survival, text_test_pred)

```

